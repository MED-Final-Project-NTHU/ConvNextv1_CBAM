# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NrTILhyaQjPUru-aBWjbwejf_M3NNXjT
"""

import torch
# from model import ResnetEncoder
# from model_convnext import ConvNeXt
# from model_convnextv2 import ConvNeXtV2
# from model_convnextv2 import convnextv2_tiny as create_model #convnextv2_base
from model_convnextv1_CBAM import convnext_tiny as create_model  #convnext_small  convnext_large
import numpy as np
import os
import random
import torch.optim as optim
import torch.nn as nn
from Myloader_multiT import Myloader
import time
import torchvision.models as models
from torchmetrics.classification import MultilabelAveragePrecision



def write_training_progress_to_file( training_info ):
    if not os.path.exists('./outputs/'):
        os.makedirs('./outputs/')

    fin_str = str(training_info)
    fin_str += "\n"

    with open("./outputs/training_progress_results.txt",'a+') as file:
        file.write(fin_str)
        

def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def evaluate(model, val_loader):
    model.eval()
    print("evaluate start~~~")
    test_running_loss = 0.0
    test_total = 0

    with torch.no_grad():
        record_target_label = torch.zeros(1, 19).to(device)
        record_predict_label = torch.zeros(1, 19).to(device)

        for (test_imgs, test_labels, test_dicoms) in val_loader:
            test_imgs = test_imgs.to(device)
            test_labels = test_labels.to(device)
            test_labels = test_labels.squeeze(-1)

            test_output = model(test_imgs)
            loss = criterion(test_output, test_labels)

            test_running_loss += loss.item() * test_imgs.size(0)
            test_total += test_imgs.size(0)

            record_target_label = torch.cat((record_target_label, test_labels), 0)
            record_predict_label = torch.cat((record_predict_label, test_output), 0)


        record_target_label = record_target_label[1::]
        record_predict_label = record_predict_label[1::]

        metric = MultilabelAveragePrecision(num_labels=19, average="macro", thresholds=None)
        mAP = metric(record_predict_label, record_target_label.to(torch.int32))
        metric_class = MultilabelAveragePrecision(num_labels=19, average=None, thresholds=None)
        mAP_per_class = metric_class(record_predict_label, record_target_label.to(torch.int32))
        
    return mAP, test_running_loss, test_total, mAP_per_class

if __name__ == "__main__":
    set_seed(123)
    

    file_path ="C:\\NTHU_清華\\11201_醫學\\Project\\"
    
    epochs = 30
    batch_size = 40
    num_classes = 19
    train_path = file_path +  "MICCAI_long_tail_train.tfrecords"
    train_index = file_path +  "MICCAI_long_tail_train.tfindex"
    val_path = file_path + "MICCAI_long_tail_val.tfrecords"
    val_index = file_path +  "MICCAI_long_tail_val.tfindex"
    test_path = file_path +  "MICCAI_long_tail_test.tfrecords"
    test_index = file_path +  "MICCAI_long_tail_test.tfindex"
    opt_lr = 1e-4
    weight_decay = 0
    training = True
    train_name = ""
    val_name = ""

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # data_transform = {
    #     "train": transforms.Compose([transforms.RandomResizedCrop(img_size),
    #                                  transforms.RandomHorizontalFlip(),
    #                                  transforms.ToTensor(),
    #                                  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),
    #     "val": transforms.Compose([transforms.Resize(int(img_size * 1.143)),
    #                                transforms.CenterCrop(img_size),
    #                                transforms.ToTensor(),
    #                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}




    # encoder = ResnetEncoder(num_layers=50, embDimension=num_classes).to(device)
    # encoder = ConvNeXt(num_classes=num_classes, depths = [3, 3, 9, 3] , dims =[96, 192, 384, 768] ).to(device)
    # encoder = ConvNeXtV2(num_classes=num_classes, depths = [3, 3, 9, 3] , dims =[96, 192, 384, 768] ).to(device)

    # encoder = convnextv2_base(pretrained=True, in_22k=True).to(device)

    encoder = create_model(num_classes=num_classes, pretrained=False).to(device)
    
    opt = optim.Adam(encoder.parameters(), lr=opt_lr, weight_decay = weight_decay)
    train_loader = Myloader(train_path, train_index, batch_size, num_workers=0, shuffle=True, )
    val_loader = Myloader(val_path, val_index, batch_size, num_workers=0, shuffle=False)
    test_loader = Myloader(test_path, test_index, 1, num_workers=0, shuffle=False) # batch_size


    criterion = nn.BCEWithLogitsLoss()

    if training == True:
#         wandb.init(
#             project='chexpert mitigate bias',
#             name= train_wandb_name)
#         config = wandb.config
#         config.batch_size = batch_size

        # weight_dir = ""
        # if not os.path.exists(weight_dir):
        #     os.makedirs(weight_dir)

        max_map = 0
        total = 0
        scaler = torch.cuda.amp.GradScaler()
        start_time = time.time()
        path = "output_training_" + str(start_time) + ".txt"
        f = open(path, 'w')
        patience = 0
        patience_setting = 5

        for epoch in range(epochs):
            

            encoder.train()
            running_loss = 0.0
            
            print("start_time:", start_time)
            count = 0
            print("epoch:",epoch)
            
            for (imgs, labels, dicom_ids) in train_loader:
                encoder.zero_grad()
                opt.zero_grad()

                imgs = imgs.to(device)
                labels = labels.to(device)
                labels = labels.squeeze(-1)

                with torch.autocast(device_type='cuda', dtype=torch.float16):
                    output = encoder(imgs)
                    loss = criterion(output, labels)

                scaler.scale(loss).backward()
                scaler.step(opt)
                scaler.update()

                running_loss += loss.item() * imgs.size(0)
                count += imgs.size(0)


                if count != 0 and count % 1000 == 0 and total == 0:
                    print(f"epoch {epoch}: {count}/unknown finished / train loss: {running_loss / count}", file=f)
                    print(f"epoch {epoch}: {count}/unknown finished / train loss: {running_loss / count}")



                elif count != 0 and count % 10 == 0 and total != 0:
                    print(f"epoch {epoch}: {count}/{total} (%.2f %%) finished / train loss: {running_loss / count}" % (count/total), file=f)
                    print(f"epoch {epoch}: {count}/{total} (%.2f %%) finished / train loss: {running_loss / count}" % (count/total))

 
            total = count

            end_time = time.time()
            print("end_time:", end_time)
            duration = end_time - start_time
            print("duration:", duration)
           


            val_mAP, val_running_loss, val_total, val_mAP_per_class = evaluate(encoder, val_loader)
            print(f"epoch {epoch} / val_mAP: {val_mAP} / val loss: {val_running_loss / val_total} / duration: {duration}", file=f)
            
            print(f"epoch {epoch} / val_mAP: {val_mAP} / val loss: {val_running_loss / val_total} / duration: {duration}")

            print(f"epoch {epoch} / val_mAP per class: {val_mAP_per_class}", file=f)
            print(f"epoch {epoch} / val_mAP per class: {val_mAP_per_class}")
            
            # training_progress = 'Train_Epoch: {} |  val_mAP:  {:.5f}  |  val loss:  {:.5f}  |  val_mAP per class: {:.5f} '.format(epoch, val_mAP , val_running_loss / val_total, val_mAP_per_class)
            # write_training_progress_to_file(training_progress)
            
            torch.save({
                    'model_state_dict': encoder.state_dict(),
                    'optimizer_state_dict': opt.state_dict(),
                }, f"model_ep_"+ str(epoch) +".pt")
            
            if val_mAP > max_map:
                max_map = val_mAP
                torch.save({
                    'model_state_dict': encoder.state_dict(),
                    'optimizer_state_dict': opt.state_dict(),
                }, f"model_best_"+ str(epoch) +".pt")
                test_mAP, test_running_loss, test_total, test_mAP_per_class = evaluate(encoder, test_loader)
                print(f"epoch {epoch} / test_mAP: {test_mAP} / test loss: {test_running_loss / test_total} ", file=f)
                print(f"epoch {epoch} / test_mAP: {test_mAP} / test loss: {test_running_loss / test_total} ")
                print(f"epoch {epoch} / test_mAP per class: {test_mAP_per_class}", file=f)
                print(f"epoch {epoch} / test_mAP per class: {test_mAP_per_class}")
                patience = 0
                # training_progress = 'Train_Epoch: {} |  test_mAP:  {:.5f}  |  test loss:  {:.5f}  |  test_mAP per class: {:.5f} '.format(epoch, test_mAP , test_running_loss / test_total, test_mAP_per_class)
                # write_training_progress_to_file(training_progress)
            elif val_mAP < max_map and patience < patience_setting:
                patience = patience + 1  
                print("current patience", patience) 

            elif patience >= patience_setting:
                break


                
        f.close()





